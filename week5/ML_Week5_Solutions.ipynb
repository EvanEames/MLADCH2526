{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dce973c",
   "metadata": {},
   "source": [
    "# ML Week 5 - Decision Trees\n",
    "##### Today we will build our second Machine Learning application from scratch (but we have already built one, so we are pros now)\n",
    "##### We will be building a supervised algorithm called a \"Decision Tree\". It can learn to efficiently classify objects using pre-determined labels.\n",
    "\n",
    "#### Much of this notebook is inspired by these two blog posts:\n",
    "* https://victorzhou.com/blog/intro-to-random-forests/\n",
    "* https://victorzhou.com/blog/gini-impurity/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd47b4e",
   "metadata": {},
   "source": [
    "### Datset\n",
    "We will again re-use our simple tools dataset from week 3.\n",
    "\n",
    "Next week we will apply a decision tree to a real world dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e5d2a",
   "metadata": {},
   "source": [
    "### How it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da946f6",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"decisionTree.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c399d",
   "metadata": {},
   "source": [
    "## Importing Common Packages\n",
    "##### You know the routine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cab3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e12c74",
   "metadata": {},
   "source": [
    "## Read in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0517933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tool data and read it into a dataframe\n",
    "df = pd.read_csv(\"./tools.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f60f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>Thickness</th>\n",
       "      <th>SharpEdgeWidth</th>\n",
       "      <th>Culture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Bear Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Bear Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Bear Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Bear Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Bear Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Beaver Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Beaver Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Beaver Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Beaver Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Beaver Culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length  Width  Thickness  SharpEdgeWidth         Culture\n",
       "0       5.1    3.5        1.4             0.2    Bear Culture\n",
       "1       4.9    3.0        1.4             0.2    Bear Culture\n",
       "2       4.7    3.2        1.3             0.2    Bear Culture\n",
       "3       4.6    3.1        1.5             0.2    Bear Culture\n",
       "4       5.0    3.6        1.4             0.2    Bear Culture\n",
       "..      ...    ...        ...             ...             ...\n",
       "145     6.7    3.0        5.2             2.3  Beaver Culture\n",
       "146     6.3    2.5        5.0             1.9  Beaver Culture\n",
       "147     6.5    3.0        5.2             2.0  Beaver Culture\n",
       "148     6.2    3.4        5.4             2.3  Beaver Culture\n",
       "149     5.9    3.0        5.1             1.8  Beaver Culture\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487e0ec",
   "metadata": {},
   "source": [
    "## Split it Into Data (X) and Labels (Y)\n",
    "##### This time we need to have the labels also as int values (so we use the label encoder function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11af276e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df[[\"Length\", \"Width\", \"Thickness\", \"SharpEdgeWidth\"]].to_numpy()\n",
    "Y = df['Culture'].to_numpy()\n",
    "\n",
    "# Cleaner way:\n",
    "# X = df.iloc[:,:-1].to_numpy()\n",
    "# Y = df.iloc[:,-1].to_numpy()\n",
    "\n",
    "Y_strings = Y\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac0dbf",
   "metadata": {},
   "source": [
    "##### We may need to recall which number correponsds to which label, so we save them in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b62529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "for i in range(len(Y)):\n",
    "    label_dict[int(Y[i])] = Y_strings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e8559e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Bear Culture', 2: 'Owl Culture', 1: 'Beaver Culture'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1db761",
   "metadata": {},
   "source": [
    "##### Our data is now ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373515e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5879a",
   "metadata": {},
   "source": [
    "## We want a measure of how \"impure\" our dataset is\n",
    "##### For this we will use \"Gini Impurity\". A perfect dataset with only 1 class would have impurity = 0, while a huge dataset where every datapoint is a different class would have be very impure (gini impurity = almost 1).\n",
    "\n",
    "##### The way to calculate Gini Impurity is asking \"if I randomly choose a data point, and then randomly classify it, what is the chance I've classified it wrong?\"\n",
    "\n",
    "##### Note: right now we only need the labels (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442b4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(Y:np.array) -> float:\n",
    "    gini = 0\n",
    "    for y in set(Y):  # For each class...\n",
    "        probability = len(Y[Y==y])/len(Y)  # What is the probability of choosing that class?\n",
    "        gini += probability*(1-probability)  # What's the probability we classify it incorrectly?\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "231cc6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the Gini Impurity of our current dataset\n",
    "\n",
    "gini(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce900357",
   "metadata": {},
   "source": [
    "## Deciding the best way to split a dataset\n",
    "\n",
    "##### We will now make a function that, given a dataset (data X and corresponding labels Y) decides the best way to split the data\n",
    "\n",
    "##### It will do this by testing all possible ways of splitting the dataset, and seeing which way reduces the impurity the most\n",
    "\n",
    "##### Note: There are much more clever and efficient ways of doing this, but the \"brute force\" method is the easiest to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def90bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function will return an integer and float.\n",
    "# This is interpreted as: split the data X based on whether column \"int\" is smaller or larger than \"float\"\n",
    "# ie. If the function returns 2, 3.5 this means all rows where column 2 is smaller than 3.5 should form one sub_dataset and all rows where column 2 is greater or equal than 3.5 should go in another\n",
    "\n",
    "def find_best_split(X:np.array, Y:np.array) -> tuple[int, float]:\n",
    "    # We will try to find the best column and value for the split. For now we initialize them to be None:\n",
    "    best_col = None\n",
    "    best_split = None\n",
    "    # The current impurity to \"beat\" is that of the entire dataset Y\n",
    "    best_gini = gini(Y)\n",
    "\n",
    "    # Now we check each column:\n",
    "    for col in range(X.shape[1]):\n",
    "        # Sort the column so we can check each split in order from smallest to largest\n",
    "        # Note we don't check the first value (hence the [1:]) because there is no datapoint with a value less than this and it will cause errors\n",
    "        X_sorted = sorted(set(X[:,col]))[1:]\n",
    "        # Now we try splitting for each value in this column\n",
    "        for x in X_sorted:\n",
    "            # here we split the dataset:\n",
    "            left_split = Y[X[:,col] < x]\n",
    "            right_split = Y[X[:,col] >= x]\n",
    "            # Then we calculate the new impurity\n",
    "            # Note: It's easy to have a pure dataset if we have very few datapoints\n",
    "            # Because of this we need to weight the left and right sides. If we have very few datapoints, this should be weighted less than if we have lots\n",
    "            new_gini = (len(left_split)/len(Y))*gini(left_split) + (len(right_split)/len(Y))*gini(right_split)\n",
    "            # If we have beat the previous gini, let's update everything:\n",
    "            if new_gini < best_gini:\n",
    "                best_gini = new_gini\n",
    "                best_col = col\n",
    "                best_split = x\n",
    "                # print(best_col, best_split, best_gini)\n",
    "    return best_col, best_split\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b46e8860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, np.float64(3.0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For our current dataset, we find that the impurity is most reduced if we split on column 2 at the value of 3.0\n",
    "\n",
    "find_best_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb27fee",
   "metadata": {},
   "source": [
    "## We now have everything we need to build our decision tree!\n",
    "\n",
    "##### The plan:\n",
    "\n",
    "##### See if our data has more than one class.\n",
    "##### If it doesn't, split it using our \"find_best_split\" function (to minimize impurity), then recursively apply the decision_tree function to both the left and right splits.\n",
    "##### If it does, we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a51c046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X:np.array, Y:np.array, level=0) -> None:\n",
    "    # If the data has more than 1 class:\n",
    "    if len(set(Y)) > 1:\n",
    "        # Find the best split\n",
    "        best_col, best_split = find_best_split(X,Y)\n",
    "        print(level, best_col, best_split)\n",
    "\n",
    "        # Split the dataset using the best split\n",
    "        left_X = X[X[:,best_col] < best_split]\n",
    "        left_Y = Y[X[:,best_col] < best_split]\n",
    "        right_X = X[X[:,best_col] >= best_split]\n",
    "        right_Y = Y[X[:,best_col] >= best_split]\n",
    "\n",
    "        # Now recursively apply decision trees to both sides:\n",
    "        print(\"going left\")\n",
    "        decision_tree(left_X, left_Y, level=level+1)\n",
    "        print(\"going right\")\n",
    "        decision_tree(right_X, right_Y, level=level+1)\n",
    "        return None\n",
    "    # If there is only a single class, we're done!\n",
    "    else:\n",
    "        print(\"leaf node reached with value \" + str(Y[0]))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55d0a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 3.0\n",
      "going left\n",
      "leaf node reached with value 0\n",
      "going right\n",
      "1 3 1.8\n",
      "going left\n",
      "2 2 5.0\n",
      "going left\n",
      "3 3 1.7\n",
      "going left\n",
      "leaf node reached with value 2\n",
      "going right\n",
      "leaf node reached with value 1\n",
      "going right\n",
      "3 3 1.6\n",
      "going left\n",
      "leaf node reached with value 1\n",
      "going right\n",
      "4 0 7.2\n",
      "going left\n",
      "leaf node reached with value 2\n",
      "going right\n",
      "leaf node reached with value 1\n",
      "going right\n",
      "2 2 4.9\n",
      "going left\n",
      "3 0 6.0\n",
      "going left\n",
      "leaf node reached with value 2\n",
      "going right\n",
      "leaf node reached with value 1\n",
      "going right\n",
      "leaf node reached with value 1\n"
     ]
    }
   ],
   "source": [
    "# Let's try it out on our data!\n",
    "decision_tree(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7cddd",
   "metadata": {},
   "source": [
    "## What if we want to apply the tree to a new tool?\n",
    "##### In general a Decision Tree should be built using something called \"object oriented coding\"\n",
    "##### It is possible to build it using if/else loops, although it's a bit awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44300a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(x):\n",
    "    if x[2] < 3.0:  # Level 0\n",
    "        return 0\n",
    "    else:\n",
    "        if x[3] < 1.8:  # Level 1\n",
    "            if x[2] < 5.0:  # Level 2\n",
    "                if x[3] < 1.7:  # Level 3\n",
    "                    return 2\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                if x[3] < 1.6:  # Level 3\n",
    "                    return 1\n",
    "                else:\n",
    "                    if x[0] < 7.2:  # Level 4\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return 1\n",
    "        else:\n",
    "            if x[2] < 4.9:  # Level 2\n",
    "                if x[0] <  6.0:  # Level 3\n",
    "                    return 2\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54f1b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Owl Culture'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_class = tree([2,3,4,2])\n",
    "label_dict[tool_class]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde26f9",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87ff48",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "##### A Decision Tree will always find a way to perfectly classify the data it's given, even if it takes 1000 splits. This can result in what's called \"overfitting\", where a decision tree learns the data \"too well\" and cannot generalize to other data.\n",
    "\n",
    "##### To fix this, we generally split the dataset into two new datasets: a training set and a testing set.\n",
    "\n",
    "##### Then, we create the decision tree with ONLY the train data, and see how well it performs on the test data.\n",
    "\n",
    "##### This gives us an idea of how well it is generalizing, and we can play with the parametres to see if we can improve it's generalizability\n",
    "\n",
    "##### In this exercise we will explore how to do this. You will likely need to consult google for many of these steps.\n",
    "\n",
    "##### Exercise 1.1 : Read in the data again like we did in the beginning, and then shuffle the rows of the dataframe randomly in order to assure the classes are evenly mixed. Save this into numpy arrays X and Y like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26325914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=0)\n",
    "X = df[[\"Length\", \"Width\", \"Thickness\", \"SharpEdgeWidth\"]].to_numpy()\n",
    "Y = df['Culture'].to_numpy()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21edea",
   "metadata": {},
   "source": [
    "##### Exercise 1.2 : Now take the first 70% of your dataset and save it into X_train and Y_train. The remaining 30% you should save into X_test and Y_test\n",
    "\n",
    "##### The 70/30 split is standard in ML, although case-dependant other ratios may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87af0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(X)*0.7)\n",
    "\n",
    "X_train = X[:split_index,:]\n",
    "Y_train = Y[:split_index]\n",
    "\n",
    "X_test = X[split_index:,:]\n",
    "Y_test = Y[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20efac3",
   "metadata": {},
   "source": [
    "##### Exercise 1.3 : Now rebuild your decision tree on only X_train and Y_train. You can do this by copying the if/else tree function above and changing the values and structure as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08acda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 3.3\n",
      "going left\n",
      "leaf node reached with value 0\n",
      "going right\n",
      "1 3 1.6\n",
      "going left\n",
      "leaf node reached with value 2\n",
      "going right\n",
      "2 3 1.8\n",
      "going left\n",
      "3 0 6.0\n",
      "going left\n",
      "leaf node reached with value 1\n",
      "going right\n",
      "4 0 7.2\n",
      "going left\n",
      "leaf node reached with value 2\n",
      "going right\n",
      "leaf node reached with value 1\n",
      "going right\n",
      "3 2 4.9\n",
      "going left\n",
      "4 0 6.0\n",
      "going left\n",
      "leaf node reached with value 2\n",
      "going right\n",
      "leaf node reached with value 1\n",
      "going right\n",
      "leaf node reached with value 1\n"
     ]
    }
   ],
   "source": [
    "decision_tree(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5056968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(x):\n",
    "    if x[2] < 3.3:  # Level 0\n",
    "        return 0\n",
    "    else:\n",
    "        if x[3] < 1.6:  # Level 1\n",
    "            return 2\n",
    "        else:\n",
    "            if x[3] < 1.8:\n",
    "                if x[0] < 6.0:\n",
    "                    return 1\n",
    "                else:\n",
    "                    if x[0] < 7.2:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return 1\n",
    "            else:\n",
    "                if x[2] < 4.9:\n",
    "                    if x[0] < 6.0:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return 1\n",
    "                else:\n",
    "                    return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78154a",
   "metadata": {},
   "source": [
    "##### Exercise 1.5 : Finally, compare the accuracy of the decision tree on X_test and Y_test. How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97acf23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.11111111111111\n"
     ]
    }
   ],
   "source": [
    "true = 0\n",
    "for x,y in zip(X_test, Y_test):\n",
    "    if tree(x) == y:\n",
    "        true += 1\n",
    "\n",
    "accuracy = true/len(Y_test)\n",
    "print(accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bcc32",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "##### To prevent the tree from overfitting we may want to stop it from going too \"deep\" (ie, only recursing a certain number of times).\n",
    "\n",
    "##### How could we modify our decision_tree function to avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51fdf21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac3be82d",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "##### As you've probably imagined, decision trees, test/train splitting, and accuracy testing are all included in the kmeans library.\n",
    "##### We will see these next week, but if you've completed the previous exercises, try already exploring the kmeans versions of decision trees and applying them to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22310365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
